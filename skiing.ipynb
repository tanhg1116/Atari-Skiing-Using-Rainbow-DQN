{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skiing\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"Image/skiing.png\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Directory and Structure](#toc1_)    \n",
    "- [Introduction](#toc2_)    \n",
    "- [Experiment Design](#toc3_)    \n",
    "- [Development of methods](#toc4_)    \n",
    "  - [Import all necessary library](#toc4_1_)    \n",
    "  - [Implement Environment Wrapper](#toc4_2_)    \n",
    "  - [Implement the Heuristic Agent](#toc4_3_)    \n",
    "  - [Training](#toc4_4_)    \n",
    "    - [Parameter Settings](#toc4_4_1_)    \n",
    "    - [Trying the Heuristic Agent](#toc4_4_2_)    \n",
    "    - [Create the Environment, Agent, Heuristic Agent, Replay Memory](#toc4_4_3_)    \n",
    "    - [Load the previous trained model](#toc4_4_4_)    \n",
    "  - [Evaluate the model (Latest checkpoint)](#toc4_5_)    \n",
    "- [Analysis of results](#toc5_)    \n",
    "- [Conclusion](#toc6_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Directory and Structure](#toc0_)\n",
    "```\n",
    "|-- skiing.ipynb # This file\n",
    "\n",
    "|-- agent.py     # Contain the Agent Class\n",
    "\n",
    "|-- memory.py    # Contain ReplayMemory Class  \n",
    "\n",
    "|-- model.py     # Contain the DQN to control the agent \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc2_'></a>[Introduction](#toc0_)\n",
    "In this experiment, we delve into the application of RL to the challenging Gym Skiing Atari environment, seeking to formulate the RL problem and design experiments to gauge the learning capabilities of a Deep Q-Network (DQN) model with __imitation learning__ by using the episode generated by heuristic agent to train the DQN. \n",
    "\n",
    "The formulation of the RL problem involves defining the state space, action space, and reward structure to guide the learning process effectively. The state, action and reward of the Skiing Environment is as follows:\n",
    "* The state at each step, $s_t$ is represented as a cropped gray scale image with shape = (130, 144). To let the DQN model have the previous state information, continuous 4 states with shape (4,130,144) is passed to the DQN model to obtain the action.\n",
    "\n",
    "* The agent has 3 possible actions and it can perform 1 action at each step, $a_t$. Since the action is discrete, the selection for DNN will be DQN as only single forward pass is needed to obtain an action. The action value and its corresponding meaning is as follows:\n",
    "\n",
    "| Value | Meaning |\n",
    "|:-:|:-:|\n",
    "| 0     | NOOP    |\n",
    "| 1     | RIGHT   |\n",
    "| 2     | LEFT    |\n",
    "\n",
    "* The reward is always -1 if the agent does not reach the goal state, and ~ -450 penalty given at the end of the game if the agent does not pass a flag in the game. In the game, there is total of 20 pair of flags. For example, if the agent does not pass 10 pairs of flag in an episode, -4500 penalty will only be given at the end of the game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc3_'></a>[Experiment Design](#toc0_)\n",
    "\n",
    "* First, the selection of the Deep Learning Model will be the Rainbow DQN and it is the same for the __Pong__ environment. The only difference is that in this case we are dealing with an image, so some convolutional layers are added in the beginning of neural network to extract the features from the images. The model architecture is shown below:\n",
    "\n",
    "    <img src=\"Image/DQN_Skiing.png\" width=600px>\n",
    "\n",
    "* Secondly, to facilitate faster training speed of Rainbow DQN, a __Heuristic Agent__ that does not have the optimal action, but can pass all the flags in each games is built to guide the training of the Rainbow DQN. The pseudocode of sampling an episode for training is as follows, with 80% probability that the action is sampled by the heuristic agent, 10% probability for a random policy to facilitate exploration, and 10% based on the action of DQN to facilitate exploitation. The policy used to select actions from DQN is based on the softmax policy.\n",
    "```\n",
    "if rand_value < 0.8:   # Sample action from heuristic agent\n",
    "    action = heuristic_agent.get_action(raw_obs)\n",
    "elif rand_value < 0.9: # Sample action from DQN\n",
    "    action          = dqn.act(state)[0] # Choose an action greedily (with noisy weights)         \n",
    "else:                  # Sample action from random policy\n",
    "    action = random.randint(0,2)\n",
    "```\n",
    "* At each 50000 steps of training, a checkpoint will be saved for further analysis. After sufficient training, 5 models with difference stages are chosen to investigate the effectiveness of the learning process. The key metrics considered for evaluation of effectiveness are the __average number of time steps taken to finish a game for n episodes__, and the __average accumulated rewards achieved for n episodes__. In our case, n is set to be 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc4_'></a>[Development of methods](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc4_1_'></a>[Import all necessary library](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-16T04:26:04.760982Z",
     "start_time": "2023-12-16T04:26:04.737914Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 2 additional library is require to running the skiing environment\n",
    "# !pip install gym[atari]\n",
    "# !pip install autorom[accept-rom-license]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-16T04:26:17.045455Z",
     "start_time": "2023-12-16T04:26:04.766298Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from IPython import display\n",
    "import os\n",
    "import sys\n",
    "from collections import deque\n",
    "\n",
    "import random\n",
    "import argparse\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import retro\n",
    "import gym\n",
    "import cv2\n",
    "import torch\n",
    "from torchsummary import summary\n",
    "\n",
    "# Pre-defined library\n",
    "from agent import Agent\n",
    "from memory import ReplayMemory\n",
    "\n",
    "sys.argv = ['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-16T04:26:17.090188Z",
     "start_time": "2023-12-16T04:26:17.057271Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.21.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gym.__version__ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc4_2_'></a>[Implement Environment Wrapper](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-16T04:26:17.136485Z",
     "start_time": "2023-12-16T04:26:17.099089Z"
    }
   },
   "outputs": [],
   "source": [
    "class Env_Wrapper(gym.Wrapper):\n",
    "    def __init__(self, env, args):\n",
    "        super(Env_Wrapper, self).__init__(env)\n",
    "        # Set up parameters \n",
    "        self.window       = args.history_length             # Number of frames to save in the history\n",
    "        self.state_buffer =  deque([], maxlen=self.window)  # Buffer to store historical frames \n",
    "        self.device       = args.device                     # Running on either 'cpu' or 'cuda'\n",
    "        self.env          = env                             # Refrence to the original environment \n",
    "        \n",
    "    def _reset_buffer(self):\n",
    "        # Reset the state buffer with zero tensors \n",
    "        for _ in range(self.window):\n",
    "            self.state_buffer.append(torch.zeros(130, 144, device=self.device))\n",
    "    \n",
    "    def process_state(self, state):\n",
    "        # Preprocess the state by converting to grayscale \n",
    "        gray = cv2.cvtColor(state[28:-52,8:152], cv2.COLOR_RGB2GRAY)\n",
    "        return torch.tensor(gray, device = self.device)\n",
    "        \n",
    "    def reset(self):\n",
    "        # Reset the state buffer and obtain the initial satet from the environment \n",
    "        self._reset_buffer()\n",
    "        raw_state       = self.env.reset()  #\n",
    "        processed_state = self.process_state(raw_state)\n",
    "        \n",
    "        # Update the state buffer with the processed state\n",
    "        self.state_buffer.append(processed_state)\n",
    "        \n",
    "        # return the raw state and the stacked state buffer where each being normalize to [0,1]\n",
    "        return raw_state, torch.stack(list(self.state_buffer) , 0) / 255\n",
    "    \n",
    "    def step(self, action):\n",
    "        # Select an action in the environment and obtain the next state \n",
    "        next_state, reward, done, info = self.env.step(action)\n",
    "        \n",
    "        # preprocess the next state\n",
    "        processed_state = self.process_state(next_state)\n",
    "        \n",
    "        # Update the state buffer with the preprocessed state\n",
    "        self.state_buffer.append(processed_state)\n",
    "        \n",
    "        # Return (next state, normalized stacked state buffer, reward, end_status, info) \n",
    "        return next_state, torch.stack(list(self.state_buffer), 0) / 255, reward, done, info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc4_3_'></a>[Implement the Heuristic Agent](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Reference: https://github.com/jaywalnut310/rl-atari-skiing/blob/master/heuristic_markovian_agent.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-16T04:26:17.184854Z",
     "start_time": "2023-12-16T04:26:17.149588Z"
    },
    "code_folding": [
     15,
     28,
     40
    ]
   },
   "outputs": [],
   "source": [
    "class Heuristic_Agent:\n",
    "    # Objects can be distinquished by RGB codes.\n",
    "    # Player: [214, 92, 92]\n",
    "    # Flags (blue): [66, 72, 200]\n",
    "    # Flags (red): [184, 50, 50]\n",
    "\n",
    "    def __init__(self, env, observe):\n",
    "        # Initialize the Heuristic Agent with the initial observation\n",
    "        self.observe                       = observe\n",
    "        self.observe_old                   = observe\n",
    "        self.agent_x_old, self.agent_y_old = self.get_pos_player()\n",
    "    \n",
    "    def get_pos_player(self):\n",
    "        # Find the position of the player in the observation\n",
    "        ids = np.where(np.sum(self.observe == [214, 92, 92], -1) == 3)  # Find out the position of the player\n",
    "        return ids[0].mean(), ids[1].mean()                        # Return the center of the player\n",
    "\n",
    "    def get_pos_flags(self): \n",
    "        # Find the position of the flags in the observation                                   \n",
    "        idx_red_flags  = np.sum(self.observe == [184, 50, 50], -1) == 3\n",
    "        idx_blue_flags = np.sum(self.observe == [66, 72, 200], -1) == 3\n",
    "\n",
    "        if np.any(idx_red_flags):   \n",
    "            ids = np.where(idx_red_flags)\n",
    "        elif np.any(idx_blue_flags):\n",
    "            ids = np.where(idx_blue_flags)\n",
    "        else:\n",
    "            raise(\"Error in locating the color of red flags and blue flags\")\n",
    "\n",
    "        return ids[0].mean(), ids[1].mean()                        # Return the center of the flags\n",
    "\n",
    "    def get_speed(self):\n",
    "        # As the vertical location of the player is not changed, \n",
    "        # I estimate the vertical speed by measuring how much frames are shifted up.\n",
    "        min_val = np.inf\n",
    "        min_idx = 0\n",
    "        for k in range(0, 7):\n",
    "            val = np.sum(np.abs(self.observe[54:-52,8:152] - self.observe_old[54+k:-52+k,8:152]))\n",
    "            if min_val > val:\n",
    "                min_idx = k\n",
    "                min_val = val\n",
    "        return min_idx\n",
    "    \n",
    "    def get_action(self, observe):\n",
    "        self.observe     = observe\n",
    "\n",
    "        self.agent_x, self.agent_y             = self.get_pos_player() # Get the position of the agent\n",
    "        self.flag_center_x, self.flag_center_y = self.get_pos_flags()  # Get the position of the flags\n",
    "        self.speed                             = self.get_speed()      # Get the speed of the agent\n",
    "\n",
    "        v_f = np.arctan2(self.flag_center_x - self.agent_x, self.flag_center_y - self.agent_y) # Direction from player to target\n",
    "        v_a = np.arctan2(self.speed, self.agent_y - self.agent_y_old)                     # speed vector of the player\n",
    "\n",
    "\n",
    "        if self.speed == 0 and (self.agent_y - self.agent_y_old) == 0:   # If the agent stop moving\n",
    "            action = np.random.choice(3, 1)[0]\n",
    "        else:\n",
    "            if v_f - v_a < -0.1:   # Move Right\n",
    "                action = 1  \n",
    "            elif v_f - v_a > 0.1:  # Move Left\n",
    "                action = 2\n",
    "            else:\n",
    "                action = 0         # Noop\n",
    "        \n",
    "        # Update the old observation\n",
    "        self.agent_y_old = self.agent_y\n",
    "        self.observe_old = observe\n",
    "    \n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-16T04:26:17.239447Z",
     "start_time": "2023-12-16T04:26:17.210491Z"
    }
   },
   "outputs": [],
   "source": [
    "def render(x, step, reward):\n",
    "    display.clear_output(wait=True)\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.clf()\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.title(f\"step: {step}, r={reward}\")\n",
    "    plt.imshow(x, cmap=plt.cm.gray)\n",
    "    plt.pause(0.0001)   # pause for plots to update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc4_4_'></a>[Training](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc4_4_1_'></a>[Parameter Settings](#toc0_)\n",
    "* Note that hyperparameters may originally be reported in ATARI game frames instead of agent steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-16T04:26:17.412889Z",
     "start_time": "2023-12-16T04:26:17.251806Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreTrueAction(option_strings=['--disable-bzip-memory'], dest='disable_bzip_memory', nargs=0, const=True, default=False, type=None, choices=None, help=\"Don't zip the memory file. Not recommended (zipping is a bit slower and much, much smaller)\", metavar=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='Rainbow')\n",
    "parser.add_argument('--num-action', type=int, default=3, help='Experiment ID')\n",
    "parser.add_argument('--id', type=str, default='default', help='Experiment ID')\n",
    "parser.add_argument('--seed', type=int, default=123, help='Random seed')\n",
    "parser.add_argument('--disable-cuda', action='store_true', help='Disable CUDA')\n",
    "# parser.add_argument('--game', type=str, default='pong', choices=atari_py.list_games(), help='ATARI game')\n",
    "parser.add_argument('--T-max', type=int, default=int(50e6), metavar='STEPS', help='Number of training steps (4x number of frames)')\n",
    "parser.add_argument('--max-episode-length', type=int, default=int(108e3), metavar='LENGTH', help='Max episode length in game frames (0 to disable)')\n",
    "parser.add_argument('--history-length', type=int, default=4, metavar='T', help='Number of consecutive states processed')\n",
    "parser.add_argument('--architecture', type=str, default='canonical', choices=['canonical', 'data-efficient'], metavar='ARCH', help='Network architecture')\n",
    "parser.add_argument('--hidden-size', type=int, default=512, metavar='SIZE', help='Network hidden size')\n",
    "parser.add_argument('--noisy-std', type=float, default=0.1, metavar='σ', help='Initial standard deviation of noisy linear layers')\n",
    "parser.add_argument('--atoms', type=int, default=51, metavar='C', help='Discretised size of value distribution')\n",
    "parser.add_argument('--V-min', type=float, default=-10, metavar='V', help='Minimum of value distribution support')\n",
    "parser.add_argument('--V-max', type=float, default=10, metavar='V', help='Maximum of value distribution support')\n",
    "# parser.add_argument('--model', type=str, metavar='PARAMS', help='Pretrained model (state dict)')\n",
    "parser.add_argument('--model', type=str, metavar='PARAMS', help='Pretrained model 1 (state dict)')\n",
    "parser.add_argument('--model2', type=str, metavar='PARAMS', help='Pretrained model 2 (state dict)')\n",
    "parser.add_argument('--memory-capacity', type=int, default=int(1e5), metavar='CAPACITY', help='Experience replay memory capacity')\n",
    "parser.add_argument('--replay-frequency', type=int, default=32, metavar='k', help='Frequency of sampling from memory')\n",
    "parser.add_argument('--priority-exponent', type=float, default=0.5, metavar='ω', help='Prioritised experience replay exponent (originally denoted α)')\n",
    "parser.add_argument('--priority-weight', type=float, default=0.4, metavar='β', help='Initial prioritised experience replay importance sampling weight')\n",
    "parser.add_argument('--multi-step', type=int, default=3, metavar='n', help='Number of steps for multi-step return')\n",
    "parser.add_argument('--discount', type=float, default=0.99, metavar='γ', help='Discount factor')\n",
    "parser.add_argument('--target-update', type=int, default=int(8e3), metavar='τ', help='Number of steps after which to update target network')\n",
    "parser.add_argument('--reward-clip', type=int, default=1, metavar='VALUE', help='Reward clipping (0 to disable)')\n",
    "parser.add_argument('--learning-rate', type=float, default=0.0000625, metavar='η', help='Learning rate')\n",
    "parser.add_argument('--adam-eps', type=float, default=1.5e-4, metavar='ε', help='Adam epsilon')\n",
    "parser.add_argument('--batch-size', type=int, default=256, metavar='SIZE', help='Batch size')\n",
    "parser.add_argument('--norm-clip', type=float, default=10, metavar='NORM', help='Max L2 norm for gradient clipping')\n",
    "parser.add_argument('--learn-start', type=int, default=int(20e3), metavar='STEPS', help='Number of steps before starting training')\n",
    "parser.add_argument('--evaluate', action='store_true', help='Evaluate only')\n",
    "parser.add_argument('--evaluation-interval', type=int, default=50000, metavar='STEPS', help='Number of training steps between evaluations')\n",
    "parser.add_argument('--evaluation-episodes', type=int, default=2, metavar='N', help='Number of evaluation episodes to average over')\n",
    "### TODO: Note that DeepMind's evaluation method is running the latest agent for 500K frames ever every 1M steps\n",
    "parser.add_argument('--evaluation-size', type=int, default=500, metavar='N', help='Number of transitions to use for validating Q')\n",
    "parser.add_argument('--render', action='store_true', help='Display screen (testing only)')\n",
    "parser.add_argument('--enable-cudnn', action='store_true', help='Enable cuDNN (faster but nondeterministic)')\n",
    "parser.add_argument('--checkpoint-interval', default=50000, help='How often to checkpoint the model, defaults to 0 (never checkpoint)')\n",
    "parser.add_argument('--memory', help='Path to save/load the memory from')\n",
    "parser.add_argument('--disable-bzip-memory', action='store_true', help='Don\\'t zip the memory file. Not recommended (zipping is a bit slower and much, much smaller)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-16T04:26:17.472757Z",
     "start_time": "2023-12-16T04:26:17.424208Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter Settings\n",
      "num_action           : 3 \n",
      "id                   : default \n",
      "seed                 : 123 \n",
      "disable_cuda         : False \n",
      "T_max                : 50000000 \n",
      "max_episode_length   : 108000 \n",
      "history_length       : 4 \n",
      "architecture         : canonical \n",
      "hidden_size          : 512 \n",
      "noisy_std            : 0.1 \n",
      "atoms                : 51 \n",
      "V_min                : -10 \n",
      "V_max                : 10 \n",
      "model                : None \n",
      "model2               : None \n",
      "memory_capacity      : 100000 \n",
      "replay_frequency     : 32 \n",
      "priority_exponent    : 0.5 \n",
      "priority_weight      : 0.4 \n",
      "multi_step           : 3 \n",
      "discount             : 0.99 \n",
      "target_update        : 8000 \n",
      "reward_clip          : 1 \n",
      "learning_rate        : 6.25e-05 \n",
      "adam_eps             : 0.00015 \n",
      "batch_size           : 256 \n",
      "norm_clip            : 10 \n",
      "learn_start          : 20000 \n",
      "evaluate             : False \n",
      "evaluation_interval  : 50000 \n",
      "evaluation_episodes  : 2 \n",
      "evaluation_size      : 500 \n",
      "render               : False \n",
      "enable_cudnn         : False \n",
      "checkpoint_interval  : 50000 \n",
      "memory               : None \n",
      "disable_bzip_memory  : False \n"
     ]
    }
   ],
   "source": [
    "# Get the arguments and create the directory if it not exist\n",
    "args = parser.parse_args()  # Parse command line arguments\n",
    "os.makedirs(\"Model\", exist_ok=True) # Create a directory named \"Model\" if it doesn't exist\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(args.seed)  # Set seed for NumPy random functions\n",
    "torch.manual_seed(np.random.randint(1, 10000)) # Set seed for PyTorch random functions\n",
    "\n",
    "# Print out each settings in the arguments\n",
    "print(\"Parameter Settings\")\n",
    "for k, v in vars(args).items():\n",
    "  print( f'{k:<20} : {str(v)} ')  # Print each parameter along with its value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-16T04:26:17.529698Z",
     "start_time": "2023-12-16T04:26:17.487922Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device used: cuda\n"
     ]
    }
   ],
   "source": [
    "# Select the device used to train the neural network\n",
    "if torch.cuda.is_available() and not args.disable_cuda:\n",
    "  # If CUDA (GPU) is available and not explicitly disabled in arguments\n",
    "  args.device = torch.device('cuda') # Set the device to CUDA\n",
    "  torch.cuda.manual_seed(np.random.randint(1, 10000)) # Set CUDA random seed\n",
    "  torch.backends.cudnn.enabled = args.enable_cudnn # Enable cuDNN if specified\n",
    "else:\n",
    "  # If CUDA is not available or explicitly disabled\n",
    "  args.device = torch.device('cpu') # Set the device to CPU\n",
    "  \n",
    "print(f\"Device used: {args.device}\") # Print the selected device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc4_4_2_'></a>[Trying the Heuristic Agent](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-16T04:26:17.571056Z",
     "start_time": "2023-12-16T04:26:17.542092Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initial state\n",
    "step             = 0\n",
    "accu_reward      = 0                             # Accumulate Reward\n",
    "env_name         = \"Skiing-v4\"\n",
    "env              = Env_Wrapper(gym.make(env_name), args)\n",
    "raw_obs, observe = env.reset()                # Initial observation\n",
    "heuristic_agent  = Heuristic_Agent(env, raw_obs) # Initialize the heuristic agent  \n",
    "\n",
    "\n",
    "while True:\n",
    "    step += 1\n",
    "    action = heuristic_agent.get_action(raw_obs)      # Get heuristic action\n",
    "        \n",
    "    raw_obs, observe, reward, done,  info = env.step(action) # Perform the action\n",
    "    accu_reward                          += reward\n",
    "    \n",
    "    # Render the environment\n",
    "    if step % 10 == 0 or done:\n",
    "        render(raw_obs[28:-52,8:152], step, accu_reward) # Crop the image it only show the agent playing\n",
    "#         render(raw_obs, step, accu_reward)\n",
    "    \n",
    "    if done: \n",
    "        break    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc4_4_3_'></a>[Create the Environment, Agent, Heuristic Agent, Replay Memory](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-16T04:26:20.356739Z",
     "start_time": "2023-12-16T04:26:17.585400Z"
    }
   },
   "outputs": [],
   "source": [
    "env_name         = \"Skiing-v4\"                           # Select Skiing version 4 as our environment\n",
    "env              = Env_Wrapper(gym.make(env_name), args) # Create the environment\n",
    "raw_obs, observe = env.reset()                           # Initial observation\n",
    "\n",
    "dqn              = Agent(args, env)                        # Create Agent \n",
    "heuristic_agent  = Heuristic_Agent(env, raw_obs)           # Create Heuristic Agent Class\n",
    "mem              = ReplayMemory(args, args.memory_capacity)# Create the memory buffer\n",
    "\n",
    "priority_weight_increase = (1 - args.priority_weight) / (args.T_max - args.learn_start) # Priority Weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "├─Sequential: 1-1                        [-1, 3, 8, 9]             --\n",
      "|    └─Conv2d: 2-1                       [-1, 32, 130, 144]        6,304\n",
      "|    └─ReLU: 2-2                         [-1, 32, 130, 144]        --\n",
      "|    └─MaxPool2d: 2-3                    [-1, 32, 65, 72]          --\n",
      "|    └─Conv2d: 2-4                       [-1, 64, 65, 72]          51,264\n",
      "|    └─ReLU: 2-5                         [-1, 64, 65, 72]          --\n",
      "|    └─MaxPool2d: 2-6                    [-1, 64, 32, 36]          --\n",
      "|    └─Conv2d: 2-7                       [-1, 64, 32, 36]          102,464\n",
      "|    └─ReLU: 2-8                         [-1, 64, 32, 36]          --\n",
      "|    └─MaxPool2d: 2-9                    [-1, 64, 16, 18]          --\n",
      "|    └─Conv2d: 2-10                      [-1, 32, 16, 18]          51,232\n",
      "|    └─ReLU: 2-11                        [-1, 32, 16, 18]          --\n",
      "|    └─MaxPool2d: 2-12                   [-1, 32, 8, 9]            --\n",
      "|    └─Conv2d: 2-13                      [-1, 3, 8, 9]             2,403\n",
      "|    └─ReLU: 2-14                        [-1, 3, 8, 9]             --\n",
      "├─NoisyLinear: 1-2                       [-1, 512]                 222,208\n",
      "├─NoisyLinear: 1-3                       [-1, 51]                  52,326\n",
      "├─NoisyLinear: 1-4                       [-1, 512]                 222,208\n",
      "├─NoisyLinear: 1-5                       [-1, 153]                 156,978\n",
      "==========================================================================================\n",
      "Total params: 867,387\n",
      "Trainable params: 867,387\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 490.78\n",
      "==========================================================================================\n",
      "Input size (MB): 0.29\n",
      "Forward/backward pass size (MB): 7.50\n",
      "Params size (MB): 3.31\n",
      "Estimated Total Size (MB): 11.09\n",
      "==========================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "├─Sequential: 1-1                        [-1, 3, 8, 9]             --\n",
       "|    └─Conv2d: 2-1                       [-1, 32, 130, 144]        6,304\n",
       "|    └─ReLU: 2-2                         [-1, 32, 130, 144]        --\n",
       "|    └─MaxPool2d: 2-3                    [-1, 32, 65, 72]          --\n",
       "|    └─Conv2d: 2-4                       [-1, 64, 65, 72]          51,264\n",
       "|    └─ReLU: 2-5                         [-1, 64, 65, 72]          --\n",
       "|    └─MaxPool2d: 2-6                    [-1, 64, 32, 36]          --\n",
       "|    └─Conv2d: 2-7                       [-1, 64, 32, 36]          102,464\n",
       "|    └─ReLU: 2-8                         [-1, 64, 32, 36]          --\n",
       "|    └─MaxPool2d: 2-9                    [-1, 64, 16, 18]          --\n",
       "|    └─Conv2d: 2-10                      [-1, 32, 16, 18]          51,232\n",
       "|    └─ReLU: 2-11                        [-1, 32, 16, 18]          --\n",
       "|    └─MaxPool2d: 2-12                   [-1, 32, 8, 9]            --\n",
       "|    └─Conv2d: 2-13                      [-1, 3, 8, 9]             2,403\n",
       "|    └─ReLU: 2-14                        [-1, 3, 8, 9]             --\n",
       "├─NoisyLinear: 1-2                       [-1, 512]                 222,208\n",
       "├─NoisyLinear: 1-3                       [-1, 51]                  52,326\n",
       "├─NoisyLinear: 1-4                       [-1, 512]                 222,208\n",
       "├─NoisyLinear: 1-5                       [-1, 153]                 156,978\n",
       "==========================================================================================\n",
       "Total params: 867,387\n",
       "Trainable params: 867,387\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 490.78\n",
       "==========================================================================================\n",
       "Input size (MB): 0.29\n",
       "Forward/backward pass size (MB): 7.50\n",
       "Params size (MB): 3.31\n",
       "Estimated Total Size (MB): 11.09\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(dqn.online_net, (4, 130, 144))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc4_4_4_'></a>[Load the previous trained model](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-16T04:26:20.388609Z",
     "start_time": "2023-12-16T04:26:20.364624Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load the model from Model/DQN_6600000.pth\n",
      "Start training at epoch 6600000\n"
     ]
    }
   ],
   "source": [
    "# Find is there any model in the \"/Model\" directory\n",
    "list_model  = os.listdir(\"Model\")\n",
    "if len(list_model) != 0:\n",
    "  # Extract the epoch numbers from model filenames\n",
    "  list_num = [int(mp[mp.index(\"_\") + 1: mp.index(\".\")]) for mp in list_model]\n",
    "  epoch_start  = np.max(list_num) # Pick the checkpoint model with the largest epoch trained \n",
    "else:\n",
    "  epoch_start = 1\n",
    "\n",
    "# Load the model if it exists\n",
    "MODEL_PATH  = f\"Model/DQN_{epoch_start}.pth\"\n",
    "if os.path.exists(MODEL_PATH):\n",
    "    print(\"Load the model from\",MODEL_PATH)\n",
    "    # Load the state dict of the neural network from the saved model file\n",
    "    dqn.online_net.load_state_dict(torch.load(MODEL_PATH, map_location=args.device))\n",
    "    dqn.update_target_net() # Update the target network with the loaded weights\n",
    "else:\n",
    "    print(\"No pre-trained model is loaded\")\n",
    "\n",
    "print(f\"Start training at epoch {epoch_start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-16T04:33:56.756009Z",
     "start_time": "2023-12-16T04:29:46.056619Z"
    }
   },
   "outputs": [],
   "source": [
    "# 4. Train the Agent\n",
    "dqn.train()       # Set the deep learning model as train() mode\n",
    "done   = True\n",
    "RENDER = False\n",
    "\n",
    "for T in trange(epoch_start, args.T_max + 1):\n",
    "    torch.cuda.empty_cache()\n",
    "    if done:   # Reset the env if it is done\n",
    "        start_T        = T\n",
    "        raw_obs, state = env.reset()\n",
    "        accu_reward    =  0\n",
    "\n",
    "    if T % args.replay_frequency == 0:  \n",
    "        dqn.reset_noise()              # Draw a new set of noisy weights\n",
    "    \n",
    "    # Sample action from heuristic agent OR DQN OR random policy\n",
    "    rand_value = random.random()\n",
    "    if rand_value < 0.8:   # Sample action from heuristic agent\n",
    "        action = heuristic_agent.get_action(raw_obs)\n",
    "    elif rand_value < 0.9: # Sample action from DQN\n",
    "        action = dqn.act(state)[0] # Choose an action based on softmax     \n",
    "    else:                  # Sample action from random policy\n",
    "        action = random.randint(0,2)\n",
    "    \n",
    "    raw_obs, next_state, reward, done,  info = env.step(action) # Perform the action\n",
    "    \n",
    "\n",
    "    # reward = max(min(reward, args.reward_clip), -args.reward_clip) # Clip reward\n",
    "    \n",
    "    accu_reward += reward\n",
    "    if RENDER:\n",
    "        if T % 10 == 0:\n",
    "            render(raw_obs, T-start_T, accu_reward)\n",
    "        \n",
    "    mem.append(state[-1], action, reward, done)  # Append transition to memory for left paddle view\n",
    " \n",
    "\n",
    "    # Train and test after accumulate enough sample from the episode\n",
    "    if T-epoch_start >= args.learn_start:\n",
    "        mem.priority_weight = min(mem.priority_weight + priority_weight_increase, 1)  # Anneal importance sampling weight β to 1\n",
    "\n",
    "        # Train the model with n-step distributional double-Q learning\n",
    "        if T % args.replay_frequency == 0:\n",
    "            dqn.learn(mem)  \n",
    "\n",
    "        # Update target network (DONE)\n",
    "        if T % args.target_update == 0: \n",
    "            dqn.update_target_net()\n",
    "\n",
    "        # Checkpoint the network (DONE)\n",
    "        if (args.checkpoint_interval != 0) and (T % args.checkpoint_interval == 0):\n",
    "            torch.save(dqn.online_net.state_dict(), f'Model/DQN_{T}.pth')\n",
    "\n",
    "    state = next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc4_5_'></a>[Evaluate the model (Latest checkpoint)](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initial state\n",
    "step             = 0\n",
    "accu_reward      = 0                             # Accumulate Reward\n",
    "env_name         = \"Skiing-v4\"\n",
    "env              = Env_Wrapper(gym.make(env_name), args)\n",
    "raw_obs, state   = env.reset()                # Initial observation\n",
    "heuristic_agent  = Heuristic_Agent(env, raw_obs) # Initialize the heuristic agent  \n",
    "\n",
    "\n",
    "while True:\n",
    "    step += 1\n",
    "\n",
    "    action = dqn.act(state)[0]      # Get action based on softmax\n",
    "        \n",
    "    raw_obs, state, reward, done,  info = env.step(action) # Perform the action\n",
    "    accu_reward                          += reward\n",
    "    \n",
    "    # Render the environment\n",
    "    if step % 10 == 0 or done:\n",
    "        render(raw_obs[28:-52,8:152], step, accu_reward) # Crop the image it only show the agent playing\n",
    "#         render(raw_obs, step, accu_reward)\n",
    "    \n",
    "    if done: \n",
    "        break    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc5_'></a>[Analysis of results](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model checkpoints</th>\n",
       "      <th>Num episodes</th>\n",
       "      <th>Average time step</th>\n",
       "      <th>Average rewards</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Model/DQN_1400000.pth</td>\n",
       "      <td>10</td>\n",
       "      <td>1600</td>\n",
       "      <td>-15945.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Model/DQN_2700000.pth</td>\n",
       "      <td>10</td>\n",
       "      <td>1671</td>\n",
       "      <td>-16898.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Model/DQN_4000000.pth</td>\n",
       "      <td>10</td>\n",
       "      <td>1766</td>\n",
       "      <td>-17415.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Model/DQN_5300000.pth</td>\n",
       "      <td>10</td>\n",
       "      <td>1595</td>\n",
       "      <td>-16418.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Model/DQN_6600000.pth</td>\n",
       "      <td>10</td>\n",
       "      <td>1550</td>\n",
       "      <td>-15530.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Model checkpoints  Num episodes  Average time step  Average rewards\n",
       "0  Model/DQN_1400000.pth            10               1600         -15945.0\n",
       "1  Model/DQN_2700000.pth            10               1671         -16898.0\n",
       "2  Model/DQN_4000000.pth            10               1766         -17415.0\n",
       "3  Model/DQN_5300000.pth            10               1595         -16418.0\n",
       "4  Model/DQN_6600000.pth            10               1550         -15530.0"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The number of episodes used to generate episodes is consistent across different checkpoints, ensuring a fair comparison of the model's performance at each stage.\n",
    "\n",
    "* The observed trend shows a gradual improvement in average rewards across training checkpoints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc6_'></a>[Conclusion](#toc0_)\n",
    "In conclusion, our analysis shows that the agent performance will gradually increase with the increasing training of the DQN. However, the __training__ is shown to be __very slow__ even though imitation learning (heuristic agent) is implemented to guide the agent while training.\n",
    "\n",
    "To facilitate faster training speed, we can consider the following:\n",
    "* Use __Policy Gradient based method__ like PPO. Since it directly optimizes the policy instead of optimizing the action value, Q, it always will have faster training speed than the Q-learning based method like Rainbow.\n",
    "* __Tune the hyperparameters__ that influence exploration and exploitation\n",
    "* __Implement some intermediate reward function__ to the agent when it passes the flags to teach the agent to pass the flag\n",
    "* Since Heuristic Agent is implemented, there is a sufficient amount of good enough 'label' for training. Some __imitation learning loss like the cross entropy can be implemented__ to facilitate faster convergence in the training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "266.341px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
